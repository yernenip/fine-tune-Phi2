{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNpn3KYCXgME7jqXzNYBeVL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yernenip/fine-tune-Phi2/blob/main/phi2_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-requisites and Model Load\n",
        "\n",
        "Before we load the model, we will need to install the packages mentioned below. These packages do not come out of the box with Google Colab.\n",
        "\n",
        "We also need to ensure that the correct runtime (for GPU) is selected. You can do this by clicking on `Runtime-->Change runtime type` in the File Menu above. For my project, I picked the T4 GPU, which comes with 16GB of CPU and GPU RAM."
      ],
      "metadata": {
        "id": "qJ-jErPJmWy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the system specs\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "m9jFFHsuq6zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQxgMZ32pzx-"
      },
      "outputs": [],
      "source": [
        "#Install the required packages for this project\n",
        "!pip install einops datasets bitsandbytes accelerate peft flash_attn\n",
        "!pip uninstall -y transformers\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install --upgrade torch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Microsoft Phi-2 Model\n",
        "The Phi-2 model is available on Hugging Face. You can read the details of it from https://huggingface.co/microsoft/phi-2\n",
        "I am also loading the model in `4-bit` which is the \"Quantization\" part of QLORA. The memory footprint of this is much smaller then the default.\n",
        "Apart from loading the model, we will also setup the tokenizer and ensure the proper settings."
      ],
      "metadata": {
        "id": "LfTAS_SRfJ6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "model_name = \"microsoft/phi-2\"\n",
        "# Configuration to load model in 4-bit quantized\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                bnb_4bit_quant_type='nf4',\n",
        "                                bnb_4bit_compute_dtype='float16',\n",
        "                                #bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "                                bnb_4bit_use_double_quant=True)\n",
        "\n",
        "\n",
        "#Loading Microsoft's Phi-2 model with compatible settings\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto',\n",
        "                                             quantization_config=bnb_config,\n",
        "                                             attn_implementation=\"flash_attention_2\",\n",
        "                                             trust_remote_code=True)\n",
        "\n",
        "# Setting up the tokenizer for Phi-2\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
        "                                          add_eos_token=True,\n",
        "                                          trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.truncation_side = \"left\"\n"
      ],
      "metadata": {
        "id": "jCpMhnh0hh0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory footprint: {model.get_memory_footprint() / 1e9} GB\")"
      ],
      "metadata": {
        "id": "ekQqS_cHK71Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login to Hugging Face\n",
        "We will login to Hugging Face, so we can save the updated model weights when training is done. Make sure to use an access key that has write permissions. You can create one from the following location.\n",
        "\n",
        "https://huggingface.co/settings/tokens"
      ],
      "metadata": {
        "id": "nKCQFdyaijhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "ByEuXEWPifTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize and Prep Dataset for Training\n",
        "\n",
        "Next, we will load the WebGLM dataset. This dataset is a web-enhanced QA system based on General Language Model (GLM). You can find more information about this at https://huggingface.co/datasets/THUDM/webglm-qa.\n",
        "\n",
        "I am taking a slice of data, as we will train our model for two epochs."
      ],
      "metadata": {
        "id": "w9d7qab_MQu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "#Load a slice of the WebGLM dataset for training and merge validation/test datasets\n",
        "train_dataset = load_dataset(\"THUDM/webglm-qa\", split=\"train[5000:10000]\")\n",
        "test_dataset = load_dataset(\"THUDM/webglm-qa\", split=\"validation+test\")\n",
        "\n",
        "print(train_dataset)\n",
        "print(test_dataset)"
      ],
      "metadata": {
        "id": "MVpqyLHqiJRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function that creates a prompt from instruction, context, category and response and tokenizes it\n",
        "def collate_and_tokenize(examples):\n",
        "\n",
        "    question = examples[\"question\"][0].replace('\"', r'\\\"')\n",
        "    answer = examples[\"answer\"][0].replace('\"', r'\\\"')\n",
        "    #unpacking the list of references and creating one string for reference\n",
        "    references = '\\n'.join([f\"[{index + 1}] {string}\" for index, string in enumerate(examples[\"references\"][0])])\n",
        "\n",
        "    #Merging into one prompt for tokenization and training\n",
        "    prompt = f\"\"\"###System:\n",
        "Read the references provided and answer the corresponding question.\n",
        "###References:\n",
        "{references}\n",
        "###Question:\n",
        "{question}\n",
        "###Answer:\n",
        "{answer}\"\"\"\n",
        "\n",
        "    #Tokenize the prompt\n",
        "    encoded = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"np\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        ## Very critical to keep max_length at 1024.\n",
        "        ## Anything more will lead to OOM on T4\n",
        "        max_length=2048,\n",
        "    )\n",
        "\n",
        "    encoded[\"labels\"] = encoded[\"input_ids\"]\n",
        "    return encoded"
      ],
      "metadata": {
        "id": "cjiMMZ-gkotJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We will just keep the input_ids and labels that we add in function above.\n",
        "columns_to_remove = [\"question\",\"answer\", \"references\"]\n",
        "\n",
        "#tokenize the training and test datasets\n",
        "tokenized_dataset_train = train_dataset.map(collate_and_tokenize,\n",
        "                                            batched=True,\n",
        "                                            batch_size=1,\n",
        "                                            remove_columns=columns_to_remove)\n",
        "tokenized_dataset_test = test_dataset.map(collate_and_tokenize,\n",
        "                                          batched=True,\n",
        "                                          batch_size=1,\n",
        "                                          remove_columns=columns_to_remove)\n"
      ],
      "metadata": {
        "id": "RZLVRL2Wk3vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check if tokenization looks good\n",
        "input_ids = tokenized_dataset_train[1]['input_ids']\n",
        "\n",
        "decoded = tokenizer.decode(input_ids, skip_special_tokens=True)\n",
        "print(decoded)"
      ],
      "metadata": {
        "id": "Tez7pYN4jqTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model\n",
        "\n",
        "We will be using LORA technique to train the model. This technique will significantly reduce the number of trainable parameters, giving better performance and memory utilization."
      ],
      "metadata": {
        "id": "rI5RazFBvt6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Accelerate training models on larger batch sizes, we can use a fully sharded data parallel model.\n",
        "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
        "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
        "\n",
        "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
        "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        ")\n",
        "\n",
        "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
      ],
      "metadata": {
        "id": "vB9NMBzasFcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "dipMfTbgPvoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "print_trainable_parameters(model)\n",
        "\n",
        "#gradient checkpointing to save memory\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Freeze base model layers and cast layernorm in fp32\n",
        "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "uamQX0JVvoGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\n",
        "    'q_proj',\n",
        "    'k_proj',\n",
        "    'v_proj',\n",
        "    'dense',\n",
        "    'fc1',\n",
        "    'fc2',\n",
        "    ], #print(model) will show the modules to use\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "lora_model = get_peft_model(model, config)\n",
        "print_trainable_parameters(lora_model)\n",
        "\n",
        "\n",
        "lora_model = accelerator.prepare_model(lora_model)"
      ],
      "metadata": {
        "id": "3PuVk7YBwxY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model and saving to Hub\n",
        "This is where, we setup the training arguments. These arguments have been carefully selected to improve memory utilization and also help increase performance. I played around with these for a while, before finalizing the following arguments.\n",
        "\n",
        "Finally, I am saving the model weights to HuggingFace Hub, so we do not loose out work. The training can run for several hours and I usually keep it running overnight."
      ],
      "metadata": {
        "id": "Ro0WaZpJhhvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',  # Output directory for checkpoints and predictions\n",
        "    overwrite_output_dir=True, # Overwrite the content of the output directory\n",
        "    per_device_train_batch_size=2,  # Batch size for training\n",
        "    per_device_eval_batch_size=2,  # Batch size for evaluation\n",
        "    gradient_accumulation_steps=5, # number of steps before optimizing\n",
        "    gradient_checkpointing=True,   # Enable gradient checkpointing\n",
        "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
        "    warmup_steps=50,  # Number of warmup steps\n",
        "    #max_steps=1000,  # Total number of training steps\n",
        "    num_train_epochs=2,  # Number of training epochs\n",
        "    learning_rate=5e-5,  # Learning rate\n",
        "    weight_decay=0.01,  # Weight decay\n",
        "    optim=\"paged_adamw_8bit\", #Keep the optimizer state and quantize it\n",
        "    fp16=True, #Use mixed precision training\n",
        "    #For logging and saving\n",
        "    logging_dir='./logs',\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,  # Limit the total number of checkpoints\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    load_best_model_at_end=True, # Load the best model at the end of training\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    train_dataset=tokenized_dataset_train,\n",
        "    eval_dataset=tokenized_dataset_test,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "#Disable cache to prevent warning, renable for inference\n",
        "#model.config.use_cache = False\n",
        "\n",
        "start_time = time.time()  # Record the start time\n",
        "trainer.train()  # Start training\n",
        "end_time = time.time()  # Record the end time\n",
        "\n",
        "training_time = end_time - start_time  # Calculate total training time\n",
        "\n",
        "print(f\"Training completed in {training_time} seconds.\")\n",
        "\n",
        "#Save model to hub to ensure we save our work.\n",
        "lora_model.push_to_hub(\"phi2-webglm-qlora\",\n",
        "                  use_auth_token=True,\n",
        "                  commit_message=\"Training Phi-2\",\n",
        "                  private=True)\n",
        "\n",
        "\n",
        "#Terminate the session so we do not incur cost\n",
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "m2T1Q6Zz9k3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save model to hub to ensure we save our work.\n",
        "lora_model.push_to_hub(\"phi2-webglm-qlora\",\n",
        "                  use_auth_token=True,\n",
        "                  commit_message=\"Training Phi-2\",\n",
        "                  private=True)"
      ],
      "metadata": {
        "id": "4YtMG1_KayQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Inference\n",
        "\n",
        "**Note**: Ensure to stop your session and reconnect and reload the model before running the code below.\n",
        "\n",
        "First we will run inference without the trained weights and check the output."
      ],
      "metadata": {
        "id": "8hFerF5h-nQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup a prompt that we can use for testing\n",
        "\n",
        "new_prompt = \"\"\"###System:\n",
        "Read the references provided and answer the corresponding question.\n",
        "###References:\n",
        "[1] For most people, the act of reading is a reward in itself. However, studies show that reading books also has benefits that range from a longer life to career success. If you’re looking for reasons to pick up a book, read on for seven science-backed reasons why reading is good for your health, relationships and happiness.\n",
        "[2] As per a study, one of the prime benefits of reading books is slowing down mental disorders such as Alzheimer’s and Dementia  It happens since reading stimulates the brain and keeps it active, which allows it to retain its power and capacity.\n",
        "[3] Another one of the benefits of reading books is that they can improve our ability to empathize with others. And empathy has many benefits – it can reduce stress, improve our relationships, and inform our moral compasses.\n",
        "[4] Here are 10 benefits of reading that illustrate the importance of reading books. When you read every day you:\n",
        "[5] Why is reading good for you? Reading is good for you because it improves your focus, memory, empathy, and communication skills. It can reduce stress, improve your mental health, and help you live longer. Reading also allows you to learn new things to help you succeed in your work and relationships.\n",
        "###Question:\n",
        "Why is reading books widely considered to be beneficial?\n",
        "###Answer:\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "OvcEl1z2wRdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(new_prompt, return_tensors=\"pt\",\n",
        "                   return_attention_mask=False,\n",
        "                   padding=True, truncation=True)\n",
        "\n",
        "inputs.to('cuda')\n",
        "\n",
        "outputs = model.generate(**inputs, repetition_penalty=1.0,\n",
        "                              max_length=1000)\n",
        "result = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "VEIU36d2lyCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, lets run the model with lora config and check inference on it."
      ],
      "metadata": {
        "id": "On4l4ueKu0PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "#Load the model weights from hub\n",
        "model_id = \"praveeny/phi2-webglm-qlora\"\n",
        "trained_model = PeftModel.from_pretrained(model, model_id)\n",
        "\n",
        "#Run inference\n",
        "outputs = trained_model.generate(**inputs, max_length=1000)\n",
        "text = tokenizer.batch_decode(outputs,skip_special_tokens=True)[0]\n",
        "print(text)"
      ],
      "metadata": {
        "id": "IqLh4Qocuzc9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}