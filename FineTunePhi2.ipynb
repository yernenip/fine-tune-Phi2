{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-requisites and Model Load\n",
        "\n",
        "Before we load the model, we will need to install the packages mentioned below. These packages do not come out of the box with Google Colab.\n",
        "\n",
        "We also need to ensure that the correct runtime (for GPU) is selected. You can do this by clicking on `Runtime-->Change runtime type` in the File Menu above. For my project, I picked the T4 GPU, which comes with 16GB of CPU and GPU RAM."
      ],
      "metadata": {
        "id": "qJ-jErPJmWy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check the system specs\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "m9jFFHsuq6zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQxgMZ32pzx-"
      },
      "outputs": [],
      "source": [
        "#Install the required packages for this project\n",
        "!pip install einops datasets bitsandbytes accelerate peft"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the Microsoft Phi-2 Model\n",
        "The Phi-2 model is available on Hugging Face. You can read the details of it from https://huggingface.co/microsoft/phi-2\n",
        "I am also loading the model in `4-bit` which is the \"Quantization\" part of QLORA. The memory footprint of this is much smaller then the default.\n",
        "Apart from loading the model, we will also setup the tokenizer and ensure the proper settings."
      ],
      "metadata": {
        "id": "LfTAS_SRfJ6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "model_name = \"microsoft/phi-2\"\n",
        "# Configuration to load model in 4-bit quantized\n",
        "bnb_config = BitsAndBytesConfig(load_in_4bit=True,\n",
        "                                bnb_4bit_quant_type='nf4',\n",
        "                                bnb_4bit_compute_dtype='float16',\n",
        "                                #bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "                                bnb_4bit_use_double_quant=True)\n",
        "\n",
        "\n",
        "#Loading Microsoft's Phi-2 model with compatible settings\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto',\n",
        "                                             quantization_config=bnb_config,\n",
        "                                             trust_remote_code=True)\n",
        "\n",
        "# Setting up the tokenizer for Phi-2\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name,\n",
        "                                          add_eos_token=True,\n",
        "                                          trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.truncation_side = \"left\"\n"
      ],
      "metadata": {
        "id": "jCpMhnh0hh0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Memory footprint: {model.get_memory_footprint() / 1e9} GB\")"
      ],
      "metadata": {
        "id": "ekQqS_cHK71Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Login to Hugging Face\n",
        "We will login to Hugging Face, so we can save the updated model weights when training is done. Make sure to use an access key that has write permissions. You can create one from the following location.\n",
        "\n",
        "https://huggingface.co/settings/tokens"
      ],
      "metadata": {
        "id": "nKCQFdyaijhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "ByEuXEWPifTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize and Prep Dataset for Training\n",
        "\n",
        "Next, we will load the databricks dolly 15K dataset. This dataset is created by employees in Databricks and contains different categories. We will use this to run a instruction fine tuning on our Phi-2 model.\n",
        "We will also split the dataset into train and test datasets and tokenize it, to be used for fine tuning."
      ],
      "metadata": {
        "id": "w9d7qab_MQu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "#Load the dataset. Dolly 15K has only the train split.\n",
        "dataset = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n"
      ],
      "metadata": {
        "id": "MVpqyLHqiJRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the Dataset to train and test, with 80% for Train and 20% for Testing\n",
        "dataset = dataset.train_test_split(test_size=0.2)\n",
        "\n",
        "print(dataset)\n",
        "\n",
        "#Reassigning to variables\n",
        "train_dataset = dataset[\"train\"]\n",
        "test_dataset = dataset[\"test\"]"
      ],
      "metadata": {
        "id": "7MQBHYB1qzT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function that creates a prompt from instruction, context, category and response and tokenizes it\n",
        "def collate_and_tokenize(examples):\n",
        "\n",
        "    instruction = examples[\"instruction\"][0].replace('\"', r'\\\"')\n",
        "    context = examples[\"context\"][0].replace('\"', r'\\\"')\n",
        "    response = examples[\"response\"][0].replace('\"', r'\\\"')\n",
        "    category = examples[\"category\"][0]\n",
        "\n",
        "    #Check if context is given for the instruction\n",
        "    if context.strip():\n",
        "        context = f\"##Context: {context}\"\n",
        "\n",
        "    #Merging into one prompt for tokenization and training\n",
        "    prompt = f\"\"\"##Instruction: {instruction}\n",
        "    ##Category: {category}\n",
        "     {context}\n",
        "     ##Response: {response}\n",
        "     ##End of Example##\n",
        "    \"\"\"\n",
        "    encoded = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"np\",\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        ## Very critical to keep max_length at 512.\n",
        "        ## Anything more like 1024 or higher will lead to OOM on T4\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    encoded[\"labels\"] = encoded[\"input_ids\"]\n",
        "    return encoded"
      ],
      "metadata": {
        "id": "cjiMMZ-gkotJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We will just keep the input_ids and labels that we add in function above.\n",
        "columns_to_remove = [\"instruction\",\"context\", \"response\", \"category\"]\n",
        "\n",
        "#tokenize the training and test datasets\n",
        "tokenized_dataset_train = train_dataset.map(collate_and_tokenize,\n",
        "                                            batched=True,\n",
        "                                            batch_size=1,\n",
        "                                            remove_columns=columns_to_remove)\n",
        "tokenized_dataset_test = test_dataset.map(collate_and_tokenize,\n",
        "                                          batched=True,\n",
        "                                          batch_size=1,\n",
        "                                          remove_columns=columns_to_remove)\n",
        "\n"
      ],
      "metadata": {
        "id": "RZLVRL2Wk3vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model\n",
        "\n",
        "We will be using LORA technique to train the model. This technique will significantly reduce the number of trainable parameters, giving better performance and memory utilization."
      ],
      "metadata": {
        "id": "rI5RazFBvt6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
        "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
        "\n",
        "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
        "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        ")\n",
        "\n",
        "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
      ],
      "metadata": {
        "id": "vB9NMBzasFcT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "#gradient checkpointing to save memory\n",
        "# Apparently Phi-2 does not support this :-(\n",
        "#model.gradient_checkpointing_enable()\n",
        "\n",
        "# Freeze base model layers and cast layernorm in fp32\n",
        "model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=False)\n",
        "print(model)"
      ],
      "metadata": {
        "id": "uamQX0JVvoGF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=['Wqkv','out_proj'], #Run print(model) to find the target_modules\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "#Commenting this for now. I do not see a significant difference in memory\n",
        "#utiization, with our without the accelerator.\n",
        "#model = accelerator.prepare_model(model)"
      ],
      "metadata": {
        "id": "3PuVk7YBwxY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Model and saving to Hub\n",
        "This is where, we setup the training arguments. These arguments have been carefully selected to improve memory utilization and also help increase performance. I played around with these for a while, before finalizing the following arguments.\n",
        "\n",
        "Finally, I am saving the model weights to HuggingFace Hub, so we do not loose out work. The training can run for several hours and I usually keep it running overnight."
      ],
      "metadata": {
        "id": "Ro0WaZpJhhvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',  # Output directory for checkpoints and predictions\n",
        "    overwrite_output_dir=True, # Overwrite the content of the output directory\n",
        "    per_device_train_batch_size=2,  # Batch size for training\n",
        "    per_device_eval_batch_size=2,  # Batch size for evaluation\n",
        "    gradient_accumulation_steps=4, # number of steps before updating weights\n",
        "    #max_steps=1000,  # Total number of training steps\n",
        "    num_train_epochs=1,  # Number of training epochs\n",
        "    learning_rate=1e-5,  # Learning rate\n",
        "    weight_decay=0.01,  # Weight decay\n",
        "    optim=\"paged_adamw_8bit\", #Keep the optimizer state and quantize it\n",
        "    fp16=True, #Use mixed precision training\n",
        "    #For logging and saving\n",
        "    logging_dir='./logs',\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=200,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    save_total_limit=2,  # Limit the total number of checkpoints\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=200,\n",
        "    load_best_model_at_end=True, # Load the best model at the end of training\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_dataset_train,\n",
        "    eval_dataset=tokenized_dataset_test,\n",
        "    args=training_args,\n",
        ")\n",
        "\n",
        "start_time = time.time()  # Record the start time\n",
        "\n",
        "trainer.train()  # Start training\n",
        "\n",
        "end_time = time.time()  # Record the end time\n",
        "training_time = end_time - start_time  # Calculate total training time\n",
        "\n",
        "print(f\"Training completed in {training_time} seconds.\")\n",
        "\n",
        "#Save model to hub to ensure we save our work.\n",
        "model.push_to_hub(\"phi2-qlora-dolly\",\n",
        "                  use_auth_token=True,\n",
        "                  commit_message=\"Training Phi-2\",\n",
        "                  private=True)"
      ],
      "metadata": {
        "id": "m2T1Q6Zz9k3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run Inference\n",
        "\n",
        "First we will run inference without the trained weights and check the output."
      ],
      "metadata": {
        "id": "8hFerF5h-nQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pick a random example from the dataset.\n",
        "#We will use the same example for trained model\n",
        "query = dataset[\"instruction\"][4500].replace('\"', r'\\\"')\n",
        "context = dataset[\"context\"][4500].replace('\"', r'\\\"')\n",
        "\n",
        "prompt = f\"instruction: {query}\\ncontext: {context}\\nresponse:\"\n",
        "print(prompt)\n"
      ],
      "metadata": {
        "id": "OvcEl1z2wRdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(prompt, return_tensors=\"pt\", return_attention_mask=False)\n",
        "\n",
        "inputs.to('cuda')\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=200)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)\n"
      ],
      "metadata": {
        "id": "VEIU36d2lyCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, lets reload the model with lora config and run inference on it."
      ],
      "metadata": {
        "id": "On4l4ueKu0PU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "#Load the model from hub\n",
        "model_id = \"praveeny/phi2-qlora-dolly\"\n",
        "config = PeftConfig.from_pretrained(model_id)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
        "lora_model = PeftModel.from_pretrained(base_model, model_id)\n",
        "\n",
        "#Run inference\n",
        "outputs = lora_model.generate(**inputs, max_length=200)\n",
        "text = tokenizer.batch_decode(outputs)[0]\n",
        "print(text)"
      ],
      "metadata": {
        "id": "IqLh4Qocuzc9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}