{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/zTG8kwBZiuuZZpYa7t05",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yernenip/fine-tune-Phi2/blob/main/Phi2_GGUF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Merging Phi-2 with fine-tuned LoRA adapters"
      ],
      "metadata": {
        "id": "8Zjxvt5M8DI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft\n",
        "!pip install --upgrade torch transformers\n"
      ],
      "metadata": {
        "id": "HCX-u_1170t5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"microsoft/phi-2\"\n",
        "torch.set_default_device(\"cuda\")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "                                             torch_dtype=torch.float16,\n",
        "                                             trust_remote_code=True)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "pFXA9qI970Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "#Load the model weights from hub\n",
        "model_adapters = \"praveeny/phi2-webglm-qlora\"\n",
        "model = PeftModel.from_pretrained(model, model_adapters)\n",
        "\n",
        "model = model.merge_and_unload()\n",
        "model.save_pretrained(\"updated_adapters\")\n"
      ],
      "metadata": {
        "id": "xEu3zt-t8MSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"phi2-webglm-guava\", private=True,\n",
        "                  commit_message=\"merged model\")\n",
        "\n",
        "tokenizer.push_to_hub(\"phi2-webglm-guava\", private=True,\n",
        "                  commit_message=\"tokenizer\")"
      ],
      "metadata": {
        "id": "_ssqNx-BAx1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up Llama.cpp and saving model in GGUF format\n",
        "\n",
        "**Note:** At this point, I would recommend disconnecting and deleting runtime. Merging the model and pushing to hub (as shown above) takes up a lot of resources.\n",
        "\n",
        "Thats why, I am installing the packages required again below."
      ],
      "metadata": {
        "id": "pA9ybK8a726a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "model_id=\"praveeny/phi2-webglm-guava\"\n",
        "#Download the repository to local_dir\n",
        "snapshot_download(repo_id=model_id, local_dir=\"phi2\",\n",
        "                  local_dir_use_symlinks=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "-ToQCFbv6Mg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup Llama.cpp and install required packages\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "!cd llama.cpp && LLAMA_CUBLAS=1 make\n",
        "!pip install -r llama.cpp/requirements.txt"
      ],
      "metadata": {
        "id": "OEtAE4oLf9oK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python llama.cpp/convert-hf-to-gguf.py phi2 --outfile \"phi2/phi2-v2-fp16.bin\" --outtype f16"
      ],
      "metadata": {
        "id": "ujCJoFKybLoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./llama.cpp/quantize \"phi2/phi2-v2-fp16.bin\" \"phi2/phi2-v2-Q5_K_M.gguf\" \"q5_k_m\""
      ],
      "metadata": {
        "id": "65Njd-k_FJPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "\n",
        "model_id = \"praveeny/phi2-webglm-gguf\"\n",
        "api.create_repo(model_id, exist_ok=True, repo_type=\"model\")\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"phi2/phi2-v2-Q5_K_M.gguf\",\n",
        "    path_in_repo=\"phi2-v2-Q5_K_M.gguf\",\n",
        "    repo_id=model_id,\n",
        ")"
      ],
      "metadata": {
        "id": "fhSPYccab5-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Inference with LangChain, Llamacpp and GGUF\n",
        "\n",
        "At this point, I would recommend to disconnect and delete the runtime. The code below can be run separately and we will redownload the GGUF file from hugging face hub, then work with the local copy.\n",
        "\n",
        "I am also running this on a CPU instance, instead of GPU."
      ],
      "metadata": {
        "id": "zuzpm19uKB4S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub\n",
        "!pip install langchain\n",
        "!pip install llama-cpp-python"
      ],
      "metadata": {
        "id": "0_1GboP-3-4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "model_id=\"praveeny/phi2-webglm-gguf\"\n",
        "#Download the repository to local_dir\n",
        "snapshot_download(repo_id=model_id, local_dir=\"phi2-gguf\",\n",
        "                  local_dir_use_symlinks=False)"
      ],
      "metadata": {
        "id": "cb_wcmZi4Z0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up LangChain and prompt"
      ],
      "metadata": {
        "id": "QGg3gGth4z4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_community.llms import LlamaCpp\n",
        "\n",
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])"
      ],
      "metadata": {
        "id": "5jZpmzVL4-eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running inference with Llamacpp"
      ],
      "metadata": {
        "id": "6maDyHiX5Frv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure the model path is correct for your system!\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"phi2-gguf/phi2-v2-Q5_K_M.gguf\",\n",
        "    temperature=0.75,\n",
        "    max_tokens=2000,\n",
        "    top_p=1,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,  # Verbose is required to pass to the callback manager\n",
        ")\n",
        "\n",
        "\n",
        "prompt = \"\"\"###System:\n",
        "Read the references provided and answer the corresponding question.\n",
        "###References:\n",
        "[1] For most people, the act of reading is a reward in itself. However, studies show that reading books also has benefits that range from a longer life to career success. If you’re looking for reasons to pick up a book, read on for seven science-backed reasons why reading is good for your health, relationships and happiness.\n",
        "[2] As per a study, one of the prime benefits of reading books is slowing down mental disorders such as Alzheimer’s and Dementia  It happens since reading stimulates the brain and keeps it active, which allows it to retain its power and capacity.\n",
        "[3] Another one of the benefits of reading books is that they can improve our ability to empathize with others. And empathy has many benefits – it can reduce stress, improve our relationships, and inform our moral compasses.\n",
        "[4] Here are 10 benefits of reading that illustrate the importance of reading books. When you read every day you:\n",
        "[5] Why is reading good for you? Reading is good for you because it improves your focus, memory, empathy, and communication skills. It can reduce stress, improve your mental health, and help you live longer. Reading also allows you to learn new things to help you succeed in your work and relationships.\n",
        "###Question:\n",
        "Why is reading books widely considered to be beneficial?\n",
        "###Answer:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "RiyK4yFi5La3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}